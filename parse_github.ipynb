{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T05:01:52.586630Z",
     "start_time": "2025-09-08T05:01:51.430640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:8890\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:8890\"\n",
    "# ucloud api key\n",
    "API_KEY = os.getenv(\"UCLOUD_API_KEY\")\n",
    "API_BASE = \"https://api.modelverse.cn/v1\"\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=API_KEY,\n",
    "    openai_api_base=API_BASE,\n",
    "    model=\"deepseek-ai/DeepSeek-V3-0324\")\n",
    "output_parser = StrOutputParser()\n"
   ],
   "id": "20b3bc1e57d17b36",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T05:01:56.493059Z",
     "start_time": "2025-09-08T05:01:56.454767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"Github/\",\n",
    "    loader_cls=lambda path: TextLoader(path, encoding=\"utf-8\")\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "docs"
   ],
   "id": "9736683d6c479a49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Github\\\\4paradigm_results.txt'}, page_content='**OpenMLDB**  \\nåˆ†ç±»ï¼š2. æ•°æ®é›†  \\nstaræ•°å­—ï¼š1662  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\né¡¹ç›®æè¿°ï¼šOpenMLDB is an open-source machine learning database that provides a feature platform computing consistent features for training and inference.  \\nurl: https://github.com/4paradigm/OpenMLDB  \\n\\n---\\n\\n**k8s-vgpu-scheduler**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š572  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\né¡¹ç›®æè¿°ï¼šOpenAIOS vGPU device plugin for Kubernetes is originated from the OpenAIOS project to virtualize GPU device memory, in order to allow applications to access larger memory space than its physical capacity. It is designed for ease of use of extended device memory for AI workloads.  \\nurl: https://github.com/4paradigm/k8s-vgpu-scheduler  \\n\\n---\\n\\n**AutoX**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š539  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-07  \\né¡¹ç›®æè¿°ï¼šAutoX is an efficient automl tool, which is mainly aimed at data mining tasks with tabular data.  \\nurl: https://github.com/4paradigm/AutoX  \\n\\n---\\n\\n**openaios-platform**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š98  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-18  \\né¡¹ç›®æè¿°ï¼šOpenAIOS is an incubating open-source distributed OS kernel based on Kubernetes for AI workloads.  OpenAIOS-Platform is an AI development platform built upon OpenAIOS for enterprises to develop and deploy AI applications for production.  \\nurl: https://github.com/4paradigm/openaios-platform  \\n\\n---\\n\\n**pafka**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š67  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2024-09-23  \\né¡¹ç›®æè¿°ï¼šPafka is originated from the OpenAIOS project to leverage an optimized tiered storage access strategy to improve overall performance for streaming/messaging system.  \\nurl: https://github.com/4paradigm/pafka  \\n\\n'),\n",
       " Document(metadata={'source': 'Github\\\\AgibotTech_results.txt'}, page_content='**agibot_x1_infer**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹  \\nstaræ•°å­—ï¼š1717  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\né¡¹ç›®æè¿°ï¼šThe inference module for AgiBot X1.  \\nurl: https://github.com/AgibotTech/agibot_x1_infer  \\n\\n**agibot_x1_train**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹  \\nstaræ•°å­—ï¼š1563  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\né¡¹ç›®æè¿°ï¼šThe reinforcement learning training code for AgiBot X1.  \\nurl: https://github.com/AgibotTech/agibot_x1_train  \\n\\n**agibot_x1_hardware**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š978  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šThe hardware design for AgiBot X1.  \\nurl: https://github.com/AgibotTech/agibot_x1_hardware  \\n\\n**Genie-Envisioner**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š240  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/AgibotTech/Genie-Envisioner  \\n\\n**genie_sim**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š273  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šThe Simulation Framework from AgiBot  \\nurl: https://github.com/AgibotTech/genie_sim  \\n\\n**EnerVerse-AC**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹  \\nstaræ•°å­—ï¼š111  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\né¡¹ç›®æè¿°ï¼šOfficial Code for EnerVerse-AC: Envisioning EmbodiedEnvironments with Action Condition  \\nurl: https://github.com/AgibotTech/EnerVerse-AC  \\n\\n**EWMBench**  \\nåˆ†ç±»ï¼š2. æ•°æ®é›†  \\nstaræ•°å­—ï¼š82  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-30  \\né¡¹ç›®æè¿°ï¼šOfficial code for EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models  \\nurl: https://github.com/AgibotTech/EWMBench'),\n",
       " Document(metadata={'source': 'Github\\\\ECNU-CILAB_results.txt'}, page_content=''),\n",
       " Document(metadata={'source': 'Github\\\\FFTAI_results.txt'}, page_content=\"**teleoperation**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š122  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03T08:10:32Z  \\né¡¹ç›®æè¿°ï¼šA.K.A. Fourier Advanced Robot Teleoperation System (F.A.R.T.S.) ğŸ’¨  \\nurl: https://github.com/FFTAI/teleoperation  \\n\\n---\\n\\n**Wiki-GRx-Deploy**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š112  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-15T10:35:39Z  \\né¡¹ç›®æè¿°ï¼šThe Wiki-GRx code repository offers a fundamental module library and example code for the GRx motion control system. This repository serves as a resource for developers and engineers looking to implement or understand the GRx's capabilities in motion.  \\nurl: https://github.com/FFTAI/Wiki-GRx-Deploy  \\n\\n---\\n\\n**fourier-lerobot**  \\nåˆ†ç±»ï¼šå…¶ä»–ï¼ˆæ— æè¿°ï¼‰  \\nstaræ•°å­—ï¼š67  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T13:11:56Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/FFTAI/fourier-lerobot  \\n\\n---\\n\\n**Wiki-GRx-Gym**  \\nåˆ†ç±»ï¼šå…¶ä»–ï¼ˆæ— æè¿°ï¼‰  \\nstaræ•°å­—ï¼š55  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-01T06:29:53Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/FFTAI/Wiki-GRx-Gym  \\n\\n---\\n\\n**Wiki-GRx-MJCF**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š53  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-28T09:23:41Z  \\né¡¹ç›®æè¿°ï¼šThis repository provide a tool to convert robot urdf file to mjcf.  \\nurl: https://github.com/FFTAI/Wiki-GRx-MJCF  \\n\\n---\\n\\n**Wiki-GRx-Pipeline**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š50  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T06:19:41Z  \\né¡¹ç›®æè¿°ï¼šThe development pipeline of Fourier Intelligence GRx Series Robot.  \\nurl: https://github.com/FFTAI/Wiki-GRx-Pipeline  \\n\\n--- \\n\\n### åˆ†ç±»è¯´æ˜ï¼š\\n- **å·¥å…·**ï¼šæ¶‰åŠæœºå™¨äººæ§åˆ¶ã€æ–‡ä»¶è½¬æ¢æˆ–è¿œç¨‹æ“ä½œç­‰å®é™…åŠŸèƒ½ï¼ˆå¦‚ `teleoperation`, `Wiki-GRx-Deploy`, `Wiki-GRx-MJCF`ï¼‰ã€‚  \\n- **å…¶ä»–**ï¼šæè¿°ä¸æ˜ç¡®æˆ–æ— æè¿°çš„é¡¹ç›®ï¼ˆå¦‚ `fourier-lerobot`, `Wiki-GRx-Gym`ï¼‰ï¼Œæˆ–æè¿°æœªæ˜ç¡®åŒ¹é…å…¶ä»–æ ‡ç­¾ï¼ˆå¦‚ `Wiki-GRx-Pipeline`ï¼‰ã€‚  \\n\\næ‰€æœ‰é¡¹ç›®å‡æ»¡è¶³ `updated_at` åœ¨ 2023 å¹´ä¹‹åçš„è¦æ±‚ã€‚\"),\n",
       " Document(metadata={'source': 'Github\\\\fudan-generative-vision_results.txt'}, page_content='**hallo**  \\nåˆ†ç±»ï¼šæ¨¡å‹ï¼ˆéŸ³é¢‘é©±åŠ¨çš„è‚–åƒå›¾åƒåŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š8571  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šHalloæ˜¯ä¸€ä¸ªé€šè¿‡åˆ†å±‚éŸ³é¢‘é©±åŠ¨å®ç°è‚–åƒå›¾åƒåŠ¨ç”»åˆæˆçš„ç”Ÿæˆæ¨¡å‹ï¼Œå…·æœ‰é«˜ä¿çœŸå’Œå¯æ§æ€§ã€‚  \\nURL: https://github.com/fudan-generative-vision/hallo  \\n\\n---\\n\\n**champ**  \\nåˆ†ç±»ï¼šæ¨¡å‹ï¼ˆåŸºäº3Då‚æ•°åŒ–å¼•å¯¼çš„äººç±»å›¾åƒåŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š4232  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šChampæ˜¯ä¸€ä¸ªå¯æ§ä¸”ä¸€è‡´çš„äººç±»å›¾åƒåŠ¨ç”»ç”Ÿæˆæ¡†æ¶ï¼Œç»“åˆ3Då‚æ•°åŒ–æŒ‡å¯¼ï¼Œé€‚ç”¨äºåŠ¨æ€åœºæ™¯ç”Ÿæˆã€‚  \\nURL: https://github.com/fudan-generative-vision/champ  \\n\\n---\\n\\n**hallo2**  \\nåˆ†ç±»ï¼šæ¨¡å‹ï¼ˆé•¿æ—¶é•¿é«˜åˆ†è¾¨ç‡éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š3600  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\né¡¹ç›®æè¿°ï¼šHallo2æ˜¯Halloçš„å‡çº§ç‰ˆï¼Œä¸“æ³¨äºé•¿æ—¶é•¿å’Œé«˜åˆ†è¾¨ç‡çš„éŸ³é¢‘é©±åŠ¨è‚–åƒåŠ¨ç”»ç”Ÿæˆï¼Œæ•ˆæœæ›´ç¨³å®šã€‚  \\nURL: https://github.com/fudan-generative-vision/hallo2  \\n\\n---\\n\\n**hallo3**  \\nåˆ†ç±»ï¼šæ¨¡å‹ï¼ˆåŸºäºè§†é¢‘æ‰©æ•£Transformerçš„é«˜åŠ¨æ€è‚–åƒåŠ¨ç”»ç”Ÿæˆæ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š1302  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šHallo3åˆ©ç”¨è§†é¢‘æ‰©æ•£Transformerå®ç°é«˜åº¦åŠ¨æ€å’Œé€¼çœŸçš„è‚–åƒå›¾åƒåŠ¨ç”»ï¼Œæ”¯æŒå¤æ‚è¿åŠ¨åˆæˆã€‚  \\nURL: https://github.com/fudan-generative-vision/hallo3  \\n\\n---\\n\\n**dynamicPDB**  \\nåˆ†ç±»ï¼šæ•°æ®é›†ï¼ˆåŠ¨æ€è›‹ç™½è´¨ç»“æ„æ•°æ®åº“ï¼‰  \\nstaræ•°å­—ï¼š762  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\né¡¹ç›®æè¿°ï¼šDynamicPDBæ˜¯ä¸€ä¸ªåŠ¨æ€è›‹ç™½è´¨æ•°æ®é“¶è¡Œï¼Œæä¾›ç»“æ„ç”Ÿç‰©å­¦é¢†åŸŸçš„é«˜è´¨é‡æ—¶åºè›‹ç™½è´¨æ•°æ®ã€‚  \\nURL: https://github.com/fudan-generative-vision/dynamicPDB  \\n\\n---\\n\\n**DicFace**  \\nåˆ†ç±»ï¼šæ¨¡å‹ï¼ˆè§†é¢‘äººè„¸ä¿®å¤çš„å˜åˆ†ç¼–ç å­¦ä¹ æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š435  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-20  \\né¡¹ç›®æè¿°ï¼šDicFaceé€šè¿‡ç‹„åˆ©å…‹é›·çº¦æŸå˜åˆ†ç¼–ç æœ¬å­¦ä¹ ï¼Œå®ç°æ—¶åºä¸€è‡´çš„è§†é¢‘äººè„¸ä¿®å¤ï¼Œé€‚ç”¨äºä½è´¨é‡è§†é¢‘å¢å¼ºã€‚  \\nURL: https://github.com/fudan-generative-vision/DicFace  \\n\\n---\\n\\n**OpenHumanVid**  \\nåˆ†ç±»ï¼šæ•°æ®é›†ï¼ˆä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆæ•°æ®é›†ï¼‰  \\nstaræ•°å­—ï¼š274  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šOpenHumanVidæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡é«˜è´¨é‡æ•°æ®é›†ï¼Œç”¨äºæå‡ä»¥äººä¸ºä¸­å¿ƒçš„è§†é¢‘ç”Ÿæˆä»»åŠ¡çš„çœŸå®æ€§å’Œå¤šæ ·æ€§ã€‚  \\nURL: https://github.com/fudan-generative-vision/OpenHumanVid  \\n\\n\\n\\næ³¨ï¼š`TemporalCodeFormer`å’Œ`PPFlow`å› `stars â‰¤ 50`è¢«è¿‡æ»¤ï¼Œ`diffusion-genAI-course`å› `stars = 46`è¢«è¿‡æ»¤ã€‚'),\n",
       " Document(metadata={'source': 'Github\\\\FudanDISC_results.txt'}, page_content='**DISC-LawLLM**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆä¸­æ–‡æ³•å¾‹å¤§æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š777  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼š[ä¸­æ–‡æ³•å¾‹å¤§æ¨¡å‹] DISC-LawLLM: an intelligent legal system powered by large language models (LLMs) to provide a wide range of legal services.  \\nurl: https://github.com/FudanDISC/DISC-LawLLM  \\n\\n---\\n\\n**DISC-FinLLM**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆä¸­æ–‡é‡‘èå¤§è¯­è¨€æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š768  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03  \\né¡¹ç›®æè¿°ï¼šDISC-FinLLMï¼Œä¸­æ–‡é‡‘èå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›é‡‘èåœºæ™¯ä¸‹ä¸“ä¸šã€æ™ºèƒ½ã€å…¨é¢çš„é‡‘èå’¨è¯¢æœåŠ¡ã€‚  \\nurl: https://github.com/FudanDISC/DISC-FinLLM  \\n\\n---\\n\\n**DISC-MedLLM**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆåŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š550  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-14  \\né¡¹ç›®æè¿°ï¼šRepository of DISC-MedLLM, it is a comprehensive solution that leverages Large Language Models (LLMs) to provide accurate and truthful medical response in end-to-end conversational healthcare services.  \\nurl: https://github.com/FudanDISC/DISC-MedLLM  \\n\\n---\\n\\n**SocialAgent**  \\nåˆ†ç±»ï¼š4. å…¶ä»–ï¼ˆç¤¾äº¤æ™ºèƒ½ä½“èµ„æºé›†åˆï¼‰  \\nstaræ•°å­—ï¼š179  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šA collection of resources that investigate social agents.  \\nurl: https://github.com/FudanDISC/SocialAgent  \\n\\n---\\n\\n**SocioVerse**  \\nåˆ†ç±»ï¼š4. å…¶ä»–ï¼ˆæ— æè¿°ï¼Œæš‚å½’ç±»ï¼‰  \\nstaræ•°å­—ï¼š119  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/FudanDISC/SocioVerse  \\n\\n\\n--- \\n\\næ³¨ï¼š  \\n1. `DISCOpen-MedBox-DialoDiagnosis`çš„staræ•°é‡ä¸º23ï¼Œä½†å› å…¶æè¿°æ˜ç¡®æåˆ°å·¥å…·å±æ€§ï¼Œä¼˜å…ˆå½’ç±»åˆ°**å·¥å…·**è€Œéæ•°æ®é›†ã€‚  \\n2. `SocioVerse`å› æ— æè¿°æš‚å½’ç±»ä¸º**å…¶ä»–**ã€‚  \\n3. æ‰€æœ‰é¡¹ç›®å‡æ»¡è¶³`updated_at` â‰¥ 2023å¹´çš„æ¡ä»¶ã€‚'),\n",
       " Document(metadata={'source': 'Github\\\\IAAR-Shanghai_results.txt'}, page_content='**SurveyX**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š898  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T12:12:42Z  \\né¡¹ç›®æè¿°ï¼šAcademic Survey Paper Generation.  \\nurl: https://github.com/IAAR-Shanghai/SurveyX  \\n\\n---\\n\\n**Awesome-Attention-Heads**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š361  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T13:59:08Z  \\né¡¹ç›®æè¿°ï¼šAn awesome repository & A comprehensive survey on interpretability of LLM attention heads.  \\nurl: https://github.com/IAAR-Shanghai/Awesome-Attention-Heads  \\n\\n---\\n\\n**CRUD_RAG**  \\nåˆ†ç±»ï¼š2. æ•°æ®é›†  \\nstaræ•°å­—ï¼š328  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-26T03:16:24Z  \\né¡¹ç›®æè¿°ï¼šCRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models.  \\nurl: https://github.com/IAAR-Shanghai/CRUD_RAG  \\n\\n---\\n\\n**Meta-Chunking**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š246  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02T09:35:41Z  \\né¡¹ç›®æè¿°ï¼šMeta-Chunking: Learning Efficient Text Segmentation via Logical Perception.  \\nurl: https://github.com/IAAR-Shanghai/Meta-Chunking  \\n\\n---\\n\\n**CTGSurvey**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š185  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-12T13:36:54Z  \\né¡¹ç›®æè¿°ï¼šControllable Text Generation for Large Language Models: A Survey.  \\nurl: https://github.com/IAAR-Shanghai/CTGSurvey  \\n\\n---\\n\\n**xFinder**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š177  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-28T01:04:17Z  \\né¡¹ç›®æè¿°ï¼š[ICLR 2025] xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation.  \\nurl: https://github.com/IAAR-Shanghai/xFinder  \\n\\n---\\n\\n**UHGEval**  \\nåˆ†ç±»ï¼š2. æ•°æ®é›†  \\nstaræ•°å­—ï¼š174  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03T01:50:14Z  \\né¡¹ç›®æè¿°ï¼š[ACL 2024] User-friendly evaluation framework: Eval Suite & Benchmarks: UHGEval, HaluEval, HalluQA, etc.  \\nurl: https://github.com/IAAR-Shanghai/UHGEval  \\n\\n---\\n\\n**QAEncoder**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹  \\nstaræ•°å­—ï¼š175  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02T09:35:40Z  \\né¡¹ç›®æè¿°ï¼š[ACL 2025 Oral] QAEncoder: Towards Aligned Representation Learning in Question Answering Systems.  \\nurl: https://github.com/IAAR-Shanghai/QAEncoder  \\n\\n---\\n\\n**ICSFSurvey**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š169  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-13T00:22:01Z  \\né¡¹ç›®æè¿°ï¼šExplore concepts like Self-Correct, Self-Refine, Self-Improve, Self-Contradict, Self-Play, and Self-Knowledge, alongside o1-like reasoning elevationğŸ“ and hallucination alleviationğŸ„.  \\nurl: https://github.com/IAAR-Shanghai/ICSFSurvey  \\n\\n---\\n\\n**xVerify**  \\nåˆ†ç±»ï¼š3. å·¥å…·  \\nstaræ•°å­—ï¼š128  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-21T03:05:33Z  \\né¡¹ç›®æè¿°ï¼šxVerify: Efficient Answer Verifier for Reasoning Model Evaluations.  \\nurl: https://github.com/IAAR-Shanghai/xVerify  \\n\\n---\\n\\n**Grimoire**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹  \\nstaræ•°å­—ï¼š117  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02T16:13:40Z  \\né¡¹ç›®æè¿°ï¼šGrimoire is All You Need for Enhancing Large Language Models.  \\nurl: https://github.com/IAAR-Shanghai/Grimoire  \\n\\n---\\n\\n**PGRAG**  \\nåˆ†ç±»ï¼š4. å…¶ä»–  \\nstaræ•°å­—ï¼š53  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-06T14:36:06Z  \\né¡¹ç›®æè¿°ï¼šPGRAG.  \\nurl: https://github.com/IAAR-Shanghai/PGRAG  \\n\\n'),\n",
       " Document(metadata={'source': 'Github\\\\infinigence_results.txt'}, page_content='**Infini-Megrez**\\nåˆ†ç±»ï¼šå…¶ä»–\\nstaræ•°å­—ï¼š326\\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T05:01:48Z\\né¡¹ç›®æè¿°ï¼šNone\\nurl: https://github.com/infinigence/Infini-Megrez\\n\\n**Infini-Megrez-Omni**\\nåˆ†ç±»ï¼šå…¶ä»–\\nstaræ•°å­—ï¼š239\\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T12:52:14Z\\né¡¹ç›®æè¿°ï¼šNone\\nurl: https://github.com/infinigence/Infini-Megrez-Omni\\n\\n**FlashOverlap**\\nåˆ†ç±»ï¼šå·¥å…·\\nstaræ•°å­—ï¼š161\\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T06:51:15Z\\né¡¹ç›®æè¿°ï¼šA lightweight design for computation-communication overlap.\\nurl: https://github.com/infinigence/FlashOverlap\\n\\n**Semi-PD**\\nåˆ†ç±»ï¼šå·¥å…·\\nstaræ•°å­—ï¼š107\\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T07:40:17Z\\né¡¹ç›®æè¿°ï¼šA prefill & decode disaggregated LLM serving framework with shared GPU memory and fine-grained compute isolation.\\nurl: https://github.com/infinigence/Semi-PD\\n\\n**LVEval**\\nåˆ†ç±»ï¼šæ•°æ®é›†\\nstaræ•°å­—ï¼š70\\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-27T08:50:57Z\\né¡¹ç›®æè¿°ï¼šRepository of LV-Eval Benchmark\\nurl: https://github.com/infinigence/LVEval'),\n",
       " Document(metadata={'source': 'Github\\\\infly-ai_results.txt'}, page_content=''),\n",
       " Document(metadata={'source': 'Github\\\\loongOpen_results.txt'}, page_content='**OpenLoong-Dyn-Control**  \\nåˆ†ç±»ï¼š3. å·¥å…·ï¼ˆæœºå™¨äººåŠ¨æ€æ§åˆ¶è½¯ä»¶åŒ…ï¼‰  \\nstaræ•°å­—ï¼š215  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šWhole-Body Dynamics Control Software Package for Humanoid Robots  \\nurl: https://github.com/loongOpen/OpenLoong-Dyn-Control  \\n\\n**Unity-RL-Playground**  \\nåˆ†ç±»ï¼š3. å·¥å…·ï¼ˆå¼ºåŒ–å­¦ä¹ å¼€å‘å·¥å…·åŒ…ï¼‰  \\nstaræ•°å­—ï¼š151  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04  \\né¡¹ç›®æè¿°ï¼šReinforcement learning and imitation learning toolkits for robotics developers and for everyone.  \\nurl: https://github.com/loongOpen/Unity-RL-Playground  \\n\\n**OpenLoong-Gymloong**  \\nåˆ†ç±»ï¼š3. å·¥å…·ï¼ˆæœºå™¨äººè®­ç»ƒå¹³å°ï¼‰  \\nstaræ•°å­—ï¼š70  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-28  \\né¡¹ç›®æè¿°ï¼šTraining Platform for Humanoid Robots  \\nurl: https://github.com/loongOpen/OpenLoong-Gymloong  \\n\\n---  \\nåˆ†ç±»ä¾æ®ï¼š  \\næ‰€æœ‰é¡¹ç›®å‡ä¸ºæœºå™¨äººå¼€å‘ç›¸å…³çš„è½¯ä»¶å·¥å…·åŒ…ï¼Œæ— æ¨¡å‹æˆ–æ•°æ®é›†æè¿°ï¼Œæ•…ç»Ÿä¸€å½’ç±»ä¸ºã€Œå·¥å…·ã€ã€‚æŒ‰ `stars` é™åºæ’åˆ—ã€‚'),\n",
       " Document(metadata={'source': 'Github\\\\MemTensor_results.txt'}, page_content='**MemOS**  \\nåˆ†ç±»ï¼šå·¥å…·ï¼ˆå„ç§å¯ç”¨äºè¾…åŠ©çš„AIå·¥å…·ï¼‰  \\nstaræ•°å­—ï¼š2413  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T01:25:18Z  \\né¡¹ç›®æè¿°ï¼šMemOS (Preview) | Intelligence Begins with Memory  \\nurlï¼šhttps://github.com/MemTensor/MemOS  \\n\\n'),\n",
       " Document(metadata={'source': 'Github\\\\minimax-ai_results.txt'}, page_content=\"**MiniMax-M1**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š2844  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T02:13:22Z  \\né¡¹ç›®æè¿°ï¼šMiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model.  \\nurl: https://github.com/MiniMax-AI/MiniMax-M1  \\n\\n---\\n\\n**MiniMax-01**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š3137  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T01:46:20Z  \\né¡¹ç›®æè¿°ï¼šThe official repo of MiniMax-Text-01 and MiniMax-VL-01, large-language-model & vision-language-model based on Linear Attention.  \\nurl: https://github.com/MiniMax-AI/MiniMax-01  \\n\\n---\\n\\n**MiniMax-MCP**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š932  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T03:57:47Z  \\né¡¹ç›®æè¿°ï¼šOfficial MiniMax Model Context Protocol (MCP) server that enables interaction with powerful Text to Speech, image generation and video generation APIs.  \\nurl: https://github.com/MiniMax-AI/MiniMax-MCP  \\n\\n---\\n\\n**One-RL-to-See-Them-All**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š311  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-25T17:28:07Z  \\né¡¹ç›®æè¿°ï¼šThe official repo of One RL to See Them All: Visual Triple Unified Reinforcement Learning.  \\nurl: https://github.com/MiniMax-AI/One-RL-to-See-Them-All  \\n\\n---\\n\\n**SynLogic**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š163  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-25T17:28:13Z  \\né¡¹ç›®æè¿°ï¼šThe official repo of SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond.  \\nurl: https://github.com/MiniMax-AI/SynLogic  \\n\\n---\\n\\n**awesome-minimax-integrations**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š54  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-04T00:59:42Z  \\né¡¹ç›®æè¿°ï¼šExplore these applications integrating MiniMax's multimodal API to see how text, vision, and speech processing capabilities are incorporated into various software.  \\nurl: https://github.com/MiniMax-AI/awesome-minimax-integrations  \\n\\n---\\n\\n**MiniMax-AI.github.io**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š52  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-01T09:21:11Z  \\né¡¹ç›®æè¿°ï¼šThe official GitHub Page for MiniMax.  \\nurl: https://github.com/MiniMax-AI/MiniMax-AI.github.io  \\n\\n---\\n\\n**MiniMax-MCP-JS**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š81  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-01T15:32:47Z  \\né¡¹ç›®æè¿°ï¼šOfficial MiniMax Model Context Protocol (MCP) JavaScript implementation that provides seamless integration with MiniMax's powerful AI capabilities including image generation, video generation, text-to-speech, and voice cloning APIs.  \\nurl: https://github.com/MiniMax-AI/MiniMax-MCP-JS  \\n\"),\n",
       " Document(metadata={'source': 'Github\\\\opengvlab_results.txt'}, page_content='### **æ¨¡å‹**  \\n1. **InternVL**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š9036  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\n   é¡¹ç›®æè¿°ï¼š[CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o. æ¥è¿‘GPT-4oè¡¨ç°çš„å¼€æºå¤šæ¨¡æ€å¯¹è¯æ¨¡å‹  \\n   url: https://github.com/OpenGVLab/InternVL  \\n\\n2. **LLaMA-Adapter**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š5895  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04  \\n   é¡¹ç›®æè¿°ï¼š[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters  \\n   url: https://github.com/OpenGVLab/LLaMA-Adapter  \\n\\n3. **DragGAN**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š4981  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03  \\n   é¡¹ç›®æè¿°ï¼šUnofficial Implementation of DragGAN - \"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\" ï¼ˆDragGAN å…¨åŠŸèƒ½å®ç°ï¼Œåœ¨çº¿Demoï¼Œæœ¬åœ°éƒ¨ç½²è¯•ç”¨ï¼Œä»£ç ã€æ¨¡å‹å·²å…¨éƒ¨å¼€æºï¼Œæ”¯æŒWindows, macOS, Linuxï¼‰  \\n   url: https://github.com/OpenGVLab/DragGAN  \\n\\n4. **InternImage**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š2727  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n   é¡¹ç›®æè¿°ï¼š[CVPR 2023 Highlight] InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions  \\n   url: https://github.com/OpenGVLab/InternImage  \\n\\n5. **Ask-Anything**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š3297  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03  \\n   é¡¹ç›®æè¿°ï¼š[CVPR2024 Highlight][VideoChatGPT] ChatGPT with video understanding! And many more supported LMs such as miniGPT4, StableLM, and MOSS.  \\n   url: https://github.com/OpenGVLab/Ask-Anything  \\n\\n6. **InternGPT**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š3217  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\n   é¡¹ç›®æè¿°ï¼šInternGPT (iGPT) is an open source demo platform where you can easily showcase your AI models. Now it supports DragGAN, ChatGPT, ImageBind, multimodal chat like GPT-4, SAM, interactive image editing, etc.  \\n   url: https://github.com/OpenGVLab/InternGPT  \\n\\n7. **InternVideo**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š2036  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n   é¡¹ç›®æè¿°ï¼š[ECCV2024] Video Foundation Models & Data for Multimodal Understanding  \\n   url: https://github.com/OpenGVLab/InternVideo  \\n\\n8. **VisionLLM**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š1103  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04  \\n   é¡¹ç›®æè¿°ï¼šVisionLLM Series  \\n   url: https://github.com/OpenGVLab/VisionLLM  \\n\\n9. **VideoMamba**  \\n   åˆ†ç±»ï¼šæ¨¡å‹  \\n   staræ•°å­—ï¼š990  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\n   é¡¹ç›®æè¿°ï¼š[ECCV2024] VideoMamba: State Space Model for Efficient Video Understanding  \\n   url: https://github.com/OpenGVLab/VideoMamba  \\n\\n10. **OmniQuant**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š845  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\n    é¡¹ç›®æè¿°ï¼š[ICLR2024 spotlight] OmniQuant is a simple and powerful quantization technique for LLMs.  \\n    url: https://github.com/OpenGVLab/OmniQuant  \\n\\n11. **SAM-Med2D**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š1031  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04  \\n    é¡¹ç›®æè¿°ï¼šOfficial implementation of SAM-Med2D  \\n    url: https://github.com/OpenGVLab/SAM-Med2D  \\n\\n12. **VideoMAEv2**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š676  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-27  \\n    é¡¹ç›®æè¿°ï¼š[CVPR 2023] VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking  \\n    url: https://github.com/OpenGVLab/VideoMAEv2  \\n\\n13. **GITM**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š633  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03  \\n    é¡¹ç›®æè¿°ï¼šGhost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory  \\n    url: https://github.com/OpenGVLab/GITM  \\n\\n14. **UniFormerV2**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š327  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-26  \\n    é¡¹ç›®æè¿°ï¼š[ICCV2023] UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer  \\n    url: https://github.com/OpenGVLab/UniFormerV2  \\n\\n15. **CaFo**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š377  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n    é¡¹ç›®æè¿°ï¼š[CVPR 2023] Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners  \\n    url: https://github.com/OpenGVLab/CaFo  \\n\\n16. **all-seeing**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š495  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-23  \\n    é¡¹ç›®æè¿°ï¼š[ICLR 2024 & ECCV 2024] The All-Seeing Projects: Towards Panoptic Visual Recognition&Understanding and General Relation Comprehension of the Open World  \\n    url: https://github.com/OpenGVLab/all-seeing  \\n\\n17. **LAMM**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š317  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-20  \\n    é¡¹ç›®æè¿°ï¼š[NeurIPS 2023 Datasets and Benchmarks Track] LAMM: Multi-Modal Large Language Models and Applications as AI Agents  \\n    url: https://github.com/OpenGVLab/LAMM  \\n\\n18. **unmasked_teacher**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š337  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-29  \\n    é¡¹ç›®æè¿°ï¼š[ICCV2023 Oral] Unmasked Teacher: Towards Training-Efficient Video Foundation Models  \\n    url: https://github.com/OpenGVLab/unmasked_teacher  \\n\\n19. **Vision-RWKV**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š493  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\n    é¡¹ç›®æè¿°ï¼š[ICLR 2025 Spotlight] Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures  \\n    url: https://github.com/OpenGVLab/Vision-RWKV  \\n\\n20. **M3I-Pretraining**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š91  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n    é¡¹ç›®æè¿°ï¼š[CVPR 2023] implementation of Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information.  \\n    url: https://github.com/OpenGVLab/M3I-Pretraining  \\n\\n21. **MM-Interleaved**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š240  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-01  \\n    é¡¹ç›®æè¿°ï¼šMM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer  \\n    url: https://github.com/OpenGVLab/MM-Interleaved  \\n\\n22. **VideoChat-Flash**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š463  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\n    é¡¹ç›®æè¿°ï¼šVideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling  \\n    url: https://github.com/OpenGVLab/VideoChat-Flash  \\n\\n23. **Mono-InternVL**  \\n    åˆ†ç±»ï¼šæ¨¡å‹  \\n    staræ•°å­—ï¼š81  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n    é¡¹ç›®æè¿°ï¼š[CVPR 2025] Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training  \\n    url: https://github.com/OpenGVLab/Mono-InternVL  \\n\\n---\\n\\n### **æ•°æ®é›†**  \\n1. **gv-benchmark**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š189  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2024-09-13  \\n   é¡¹ç›®æè¿°ï¼šGeneral Vision Benchmark, GV-B, a project from OpenGVLab  \\n   url: https://github.com/OpenGVLab/gv-benchmark  \\n\\n2. **EgoExoLearn**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š69  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\n   é¡¹ç›®æè¿°ï¼š[CVPR 2024] Data and benchmark code for the EgoExoLearn dataset  \\n   url: https://github.com/OpenGVLab/EgoExoLearn  \\n\\n3. **OmniCorpus**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š391  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n   é¡¹ç›®æè¿°ï¼š[ICLR 2025 Spotlight] OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text  \\n   url: https://github.com/OpenGVLab/OmniCorpus  \\n\\n4. **MMT-Bench**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š113  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-30  \\n   é¡¹ç›®æè¿°ï¼š[ICML 2024] | MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI  \\n   url: https://github.com/OpenGVLab/MMT-Bench  \\n\\n5. **MM-NIAH**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š115  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-14  \\n   é¡¹ç›®æè¿°ï¼š[NeurIPS 2024] Needle In A Multimodal Haystack (MM-NIAH): A comprehensive benchmark designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents.  \\n   url: https://github.com/OpenGVLab/MM-NIAH  \\n\\n6. **GUI-Odyssey**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š129  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04  \\n   é¡¹ç›®æè¿°ï¼š[ICCV 2025] GUIOdyssey is a comprehensive dataset for training and evaluating cross-app navigation agents.  \\n   url: https://github.com/OpenGVLab/GUI-Odyssey  \\n\\n7. **PhyGenBench**  \\n   åˆ†ç±»ï¼šæ•°æ®é›†  \\n   staræ•°å­—ï¼š117  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-07  \\n   é¡¹ç›®æè¿°ï¼š[ICML2025] The code and data of Paper: Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation  \\n   url: https://github.com/OpenGVLab/PhyGenBench  \\n\\n---\\n\\n### **å·¥å…·**  \\n1. **Multi-Modality-Arena**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š535  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-24  \\n   é¡¹ç›®æè¿°ï¼šChatbot Arena meets multi-modality! Multi-Modality Arena allows you to benchmark vision-language models side-by-side while providing images as inputs. Supports MiniGPT-4, LLaMA-Adapter V2, LLaVA, BLIP-2, and many more!  \\n   url: https://github.com/OpenGVLab/Multi-Modality-Arena  \\n\\n2. **Instruct2Act**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š369  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\n   é¡¹ç›®æè¿°ï¼šInstruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model  \\n   url: https://github.com/OpenGVLab/Instruct2Act  \\n\\n3. **DiffRate**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š100  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n   é¡¹ç›®æè¿°ï¼š[ICCV 23]An approach to enhance the efficiency of Vision Transformer (ViT) by concurrently employing token pruning and token merging techniques, while incorporating a differentiable compression rate.  \\n   url: https://github.com/OpenGVLab/DiffRate  \\n\\n4. **ControlLLM**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š193  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-14  \\n   é¡¹ç›®æè¿°ï¼šControlLLM: Augment Language Models with Tools by Searching on Graphs  \\n   url: https://github.com/OpenGVLab/ControlLLM  \\n\\n5. **Awesome-DragGAN**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š85  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-06-15  \\n   é¡¹ç›®æè¿°ï¼šAwesome-DragGAN: A curated list of papers, tutorials, repositories related to DragGAN  \\n   url: https://github.com/OpenGVLab/Awesome-DragGAN  \\n\\n6. **Awesome-LLM4Tool**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š68  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-07  \\n   é¡¹ç›®æè¿°ï¼šA curated list of the papers, repositories, tutorials, and anythings related to the large language models for tools  \\n   url: https://github.com/OpenGVLab/Awesome-LLM4Tool  \\n\\n7. **ZeroGUI**  \\n   åˆ†ç±»ï¼šå·¥å…·  \\n   staræ•°å­—ï¼š85  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-01  \\n   é¡¹ç›®æè¿°ï¼šZeroGUI: Automating Online GUI Learning at Zero Human Cost  \\n   url: https://github.com/OpenGVLab/ZeroGUI  \\n\\n---\\n\\n### **å…¶ä»–**  \\n1. **HumanBench**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š247  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-06-16  \\n   é¡¹ç›®æè¿°ï¼šThis repo is official implementation of HumanBench (CVPR2023)  \\n   url: https://github.com/OpenGVLab/HumanBench  \\n\\n2. **UniHCP**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š156  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-29  \\n   é¡¹ç›®æè¿°ï¼šOfficial PyTorch implementation of UniHCP  \\n   url: https://github.com/OpenGVLab/UniHCP  \\n\\n3. **LORIS**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š61  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-28  \\n   é¡¹ç›®æè¿°ï¼š[ICML2023] Long-Term Rhythmic Video Soundtracker  \\n   url: https://github.com/OpenGVLab/LORIS  \\n\\n4. **MUTR**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š82  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-07  \\n   é¡¹ç›®æè¿°ï¼šã€ŒAAAI 2024ã€ Referred by Multi-Modality: A Unified Temporal Transformers for Video Object Segmentation  \\n   url: https://github.com/OpenGVLab/MUTR  \\n\\n5. **DDPS**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š72  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-18  \\n   é¡¹ç›®æè¿°ï¼šOfficial Implementation of \"Denoising Diffusion Semantic Segmentation with Mask Prior Modeling\"  \\n   url: https://github.com/OpenGVLab/DDPS  \\n\\n6. **DCNv4**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š664  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n   é¡¹ç›®æè¿°ï¼š[CVPR 2024] Deformable Convolution v4  \\n   url: https://github.com/OpenGVLab/DCNv4  \\n\\n7. **ChartAst**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š124  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-31  \\n   é¡¹ç›®æè¿°ï¼š[ACL 2024] ChartAssistant is a chart-based vision-language model for universal chart comprehension and reasoning.  \\n   url: https://github.com/OpenGVLab/ChartAst  \\n\\n8. **Hulk**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š137  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-22  \\n   é¡¹ç›®æè¿°ï¼šAn official implementation of \"Hulk: A Universal Knowledge Translator for Human-Centric Tasks\"  \\n   url: https://github.com/OpenGVLab/Hulk  \\n\\n9. **PonderV2**  \\n   åˆ†ç±»ï¼šå…¶ä»–  \\n   staræ•°å­—ï¼š359  \\n   æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-15  \\n   é¡¹ç›®æè¿°ï¼š[T-PAMI 2025] PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm  \\n   url: https://github.com/OpenGVLab/PonderV2  \\n\\n10. **LCL**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š70  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-12  \\n    é¡¹ç›®æè¿°ï¼š[NeurIPS 2024] Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning  \\n    url: https://github.com/OpenGVLab/LCL  \\n\\n11. **EfficientQAT**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š300  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n    é¡¹ç›®æè¿°ï¼š[ACL 2025 Main] EfficientQAT: Efficient Quantization-Aware Training for Large Language Models  \\n    url: https://github.com/OpenGVLab/EfficientQAT  \\n\\n12. **Diffree**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š239  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-04  \\n    é¡¹ç›®æè¿°ï¼šDiffree: Text-Guided Shape Free Object Inpainting with Diffusion Model  \\n    url: https://github.com/OpenGVLab/Diffree  \\n\\n13. **MMIU**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š86  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\n    é¡¹ç›®æè¿°ï¼š[ICLR2025] MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models  \\n    url: https://github.com/OpenGVLab/MMIU  \\n\\n14. **vinci**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š71  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\n    é¡¹ç›®æè¿°ï¼šVinci: A Real-time Embodied Smart Assistant based on Egocentric Vision-Language Model  \\n    url: https://github.com/OpenGVLab/vinci  \\n\\n15. **V2PE**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š56  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\n    é¡¹ç›®æè¿°ï¼š[ArXiv] V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding  \\n    url: https://github.com/OpenGVLab/V2PE  \\n\\n16. **TPO**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š58  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03  \\n    é¡¹ç›®æè¿°ï¼šTask Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment  \\n    url: https://github.com/OpenGVLab/TPO  \\n\\n17. **VideoChat-R1**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š182  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\n    é¡¹ç›®æè¿°ï¼šVideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning  \\n    url: https://github.com/OpenGVLab/VideoChat-R1  \\n\\n18. **VeBrain**  \\n    åˆ†ç±»ï¼šå…¶ä»–  \\n    staræ•°å­—ï¼š80  \\n    æœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-31  \\n    é¡¹ç›®æè¿°ï¼šVisual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces  \\n    url: https://github.com/OpenGVLab/VeBrain  \\n\\n--- \\n\\næ³¨ï¼š`efficient-video-recognition`ã€`STM-Evaluation`ã€`DriveMLM` å› æ— æè¿°æˆ–æ›´æ–°æ—¶é—´ä¸ç¬¦åˆè¦æ±‚è¢«çœç•¥ã€‚'),\n",
       " Document(metadata={'source': 'Github\\\\OpenMOSS_results.txt'}, page_content='**MOSS**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š12060  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\né¡¹ç›®æè¿°ï¼šå¤æ—¦å¤§å­¦å¼€æºçš„å¢å¼ºå·¥å…·å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚  \\nurl: https://github.com/OpenMOSS/MOSS  \\n\\n---\\n\\n**MOSS-TTSD**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š941  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šæ”¯æŒä¸­è‹±æ–‡è¡¨è¾¾æ€§å¯¹è¯è¯­éŸ³åˆæˆçš„æ¨¡å‹ï¼Œå…·å¤‡é›¶æ ·æœ¬å¤šè¯´è¯äººå…‹éš†å’Œé•¿æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚  \\nurl: https://github.com/OpenMOSS/MOSS-TTSD  \\n\\n---\\n\\n**AnyGPT**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š861  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05  \\né¡¹ç›®æè¿°ï¼šåŸºäºç¦»æ•£åºåˆ—å»ºæ¨¡çš„ç»Ÿä¸€å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ã€‚  \\nurl: https://github.com/OpenMOSS/AnyGPT  \\n\\n---\\n\\n**CoLLiE**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š416  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-20  \\né¡¹ç›®æè¿°ï¼šé«˜æ•ˆåä½œè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚  \\nurl: https://github.com/OpenMOSS/CoLLiE  \\n\\n---\\n\\n**SpeechGPT-2.0-preview**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š354  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02  \\né¡¹ç›®æè¿°ï¼šè¾¾åˆ°GPT-4oæ°´å¹³çš„å®æ—¶å£è¯­å¯¹è¯ç³»ç»Ÿã€‚  \\nurl: https://github.com/OpenMOSS/SpeechGPT-2.0-preview  \\n\\n---\\n\\n**VLABench**  \\nåˆ†ç±»ï¼šæ•°æ®é›†  \\nstaræ•°å­—ï¼š283  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04  \\né¡¹ç›®æè¿°ï¼šç”¨äºå…¬å¹³è¯„ä¼°è§†è§‰è¯­è¨€ä»£ç†ï¼ˆVLAï¼‰ã€å…·èº«æ™ºèƒ½ä½“å’Œå¤šæ¨¡æ€æ¨¡å‹çš„å¤§è§„æ¨¡åŸºå‡†ã€‚  \\nurl: https://github.com/OpenMOSS/VLABench  \\n\\n---\\n\\n**Language-Model-SAEs**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š146  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-30  \\né¡¹ç›®æè¿°ï¼šä¸“æ³¨äºç¨€ç–è‡ªç¼–ç å™¨ï¼ˆSAEï¼‰åœ¨è¯­è¨€æ¨¡å‹å¯è§£é‡Šæ€§ä¸­çš„ç ”ç©¶å·¥å…·ã€‚  \\nurl: https://github.com/OpenMOSS/Language-Model-SAEs  \\n\\n---\\n\\n**HalluQA**  \\nåˆ†ç±»ï¼šæ•°æ®é›†  \\nstaræ•°å­—ï¼š132  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-07  \\né¡¹ç›®æè¿°ï¼šç”¨äºè¯„ä¼°ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹å¹»è§‰ç°è±¡çš„æ•°æ®é›†å’Œè„šæœ¬ã€‚  \\nurl: https://github.com/OpenMOSS/HalluQA  \\n\\n---\\n\\n**Say-I-Dont-Know**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š83  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-14  \\né¡¹ç›®æè¿°ï¼šç ”ç©¶AIåŠ©æ‰‹èƒ½å¦è¯†åˆ«è‡ªèº«çŸ¥è¯†ç›²åŒºçš„é¡¹ç›®ï¼ˆICML 2024ï¼‰ã€‚  \\nurl: https://github.com/OpenMOSS/Say-I-Dont-Know  \\n\\n---\\n\\n**GAOKAO-MM**  \\nåˆ†ç±»ï¼šæ•°æ®é›†  \\nstaræ•°å­—ï¼š65  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-07  \\né¡¹ç›®æè¿°ï¼šä¸­æ–‡å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°åŸºå‡†ï¼ˆACL 2024 Findingsï¼‰ã€‚  \\nurl: https://github.com/OpenMOSS/GAOKAO-MM  \\n\\n---\\n\\n**Thus-Spake-Long-Context-LLM**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š56  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-20  \\né¡¹ç›®æè¿°ï¼šä»æ¶æ„ã€åŸºç¡€è®¾æ–½ã€è®­ç»ƒå’Œè¯„ä¼°å››ä¸ªè§’åº¦åˆ†æé•¿ä¸Šä¸‹æ–‡å¤§è¯­è¨€æ¨¡å‹çš„ç»¼è¿°ã€‚  \\nurl: https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM  \\n'),\n",
       " Document(metadata={'source': 'Github\\\\OpenSenseNova_results.txt'}, page_content='**piccolo-embedding**  \\nåˆ†ç±»ï¼šæ¨¡å‹ï¼ˆåµŒå…¥æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š138  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-28  \\né¡¹ç›®æè¿°ï¼šcode for piccolo embedding model from SenseTime  \\nurl: https://github.com/OpenSenseNova/piccolo-embedding  \\n'),\n",
       " Document(metadata={'source': 'Github\\\\stepfun-ai_results.txt'}, page_content='**Step-Audio**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š4502  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T23:36:12Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/stepfun-ai/Step-Audio  \\n\\n**Step-Video-T2V**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š3105  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T06:50:31Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/stepfun-ai/Step-Video-T2V  \\n\\n**Step1X-Edit**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š1615  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T02:47:17Z  \\né¡¹ç›®æè¿°ï¼šA SOTA open-source image editing model, which aims to provide comparable performance against the closed-source models like GPT-4o and Gemini 2 Flash.  \\nurl: https://github.com/stepfun-ai/Step1X-Edit  \\n\\n**Step-Audio2**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š889  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T07:23:36Z  \\né¡¹ç›®æè¿°ï¼šStep-Audio 2 is an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation.  \\nurl: https://github.com/stepfun-ai/Step-Audio2  \\n\\n**Step1X-3D**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š774  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T01:12:25Z  \\né¡¹ç›®æè¿°ï¼šStep1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets  \\nurl: https://github.com/stepfun-ai/Step1X-3D  \\n\\n**NextStep-1**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š526  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T11:03:37Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/stepfun-ai/NextStep-1  \\n\\n**Step3**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š418  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T06:49:25Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/stepfun-ai/Step3  \\n\\n**Step-Video-TI2V**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š352  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02T02:47:08Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/stepfun-ai/Step-Video-TI2V  \\n\\n**StepMesh**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š289  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T03:55:45Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/stepfun-ai/StepMesh'),\n",
       " Document(metadata={'source': 'Github\\\\Thinklab-SJTU_results.txt'}, page_content='**Awesome-LLM4AD**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š1454  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06T02:56:37Z  \\né¡¹ç›®æè¿°ï¼šA curated list of awesome LLM/VLM/VLA for Autonomous Driving(LLM4AD) resources (continually updated)  \\nurl: https://github.com/Thinklab-SJTU/Awesome-LLM4AD  \\n\\n**Bench2Drive**  \\nåˆ†ç±»ï¼šæ•°æ®é›†  \\nstaræ•°å­—ï¼š1613  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T15:22:52Z  \\né¡¹ç›®æè¿°ï¼š[NeurIPS 2024 Datasets and Benchmarks Track] Closed-Loop E2E-AD Benchmark Enhanced by World Model RL Expert  \\nurl: https://github.com/Thinklab-SJTU/Bench2Drive  \\n\\n**awesome-ml4co**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š1950  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02T02:34:31Z  \\né¡¹ç›®æè¿°ï¼šAwesome machine learning for combinatorial optzimization papers.  \\nurl: https://github.com/Thinklab-SJTU/awesome-ml4co  \\n\\n**ThinkMatch**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š872  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-31T14:47:22Z  \\né¡¹ç›®æè¿°ï¼šA research protocol for deep graph matching.  \\nurl: https://github.com/Thinklab-SJTU/ThinkMatch  \\n\\n**Crossformer**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š601  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-28T23:09:16Z  \\né¡¹ç›®æè¿°ï¼šOfficial implementation of our ICLR 2023 paper \"Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting\"  \\nurl: https://github.com/Thinklab-SJTU/Crossformer  \\n\\n**R3Det_Tensorflow**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š546  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-03-06T05:52:31Z  \\né¡¹ç›®æè¿°ï¼šCode for AAAI 2021 paper: R3Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object  \\nurl: https://github.com/Thinklab-SJTU/R3Det_Tensorflow  \\n\\n**pygmtools**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š339  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T03:36:44Z  \\né¡¹ç›®æè¿°ï¼šA Python Graph Matching Toolkit.  \\nurl: https://github.com/Thinklab-SJTU/pygmtools  \\n\\n**EDA-AI**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š268  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03T03:02:47Z  \\né¡¹ç›®æè¿°ï¼šImplementations of DeepPlace, PRNet, HubRouter, PreRoutGNN, FlexPlanner and DSBRouter.  \\nurl: https://github.com/Thinklab-SJTU/EDA-AI  \\n\\n**Awesome-LLM4EDA**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š227  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T17:29:16Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/Thinklab-SJTU/Awesome-LLM4EDA  \\n\\n**awesome-ai4eda**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š177  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T09:58:33Z  \\né¡¹ç›®æè¿°ï¼šAwesome Artificial Intelligence for Electronic Design Automation Papers.  \\nurl: https://github.com/Thinklab-SJTU/awesome-ai4eda  \\n\\n**Bench2Drive-VL**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š150  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03T04:04:48Z  \\né¡¹ç›®æè¿°ï¼šAdapting VLMs to Bench2Drive.  \\nurl: https://github.com/Thinklab-SJTU/Bench2Drive-VL  \\n\\n**Bench2DriveZoo**  \\nåˆ†ç±»ï¼šæ•°æ®é›†  \\nstaræ•°å­—ï¼š293  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-05T02:26:45Z  \\né¡¹ç›®æè¿°ï¼šBEVFormer, UniAD, VAD in Closed-Loop CARLA Evaluation with World Model RL Expert Think2Drive  \\nurl: https://github.com/Thinklab-SJTU/Bench2DriveZoo  \\n\\n**CSL_RetinaNet_Tensorflow**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š192  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-06-13T20:11:34Z  \\né¡¹ç›®æè¿°ï¼šCode for ECCV 2020 paper: Arbitrary-Oriented Object Detection with Circular Smooth Label  \\nurl: https://github.com/Thinklab-SJTU/CSL_RetinaNet_Tensorflow  \\n\\n**awesome-molecular-docking**  \\nåˆ†ç±»ï¼šå…¶ä»–  \\nstaræ•°å­—ï¼š102  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-06-29T05:40:34Z  \\né¡¹ç›®æè¿°ï¼šWe would like to maintain a list of resources which aim to solve molecular docking and other closely related tasks.  \\nurl: https://github.com/Thinklab-SJTU/awesome-molecular-docking  \\n\\n**PPO-BiHyb**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š100  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-05-30T06:38:38Z  \\né¡¹ç›®æè¿°ï¼šImplementation of our NeurIPS 2021 paper \"A Bi-Level Framework for Learning to Solve Combinatorial Optimization on Graphs\".  \\nurl: https://github.com/Thinklab-SJTU/PPO-BiHyb  \\n\\n**DriveTransformer**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š124  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03T12:16:09Z  \\né¡¹ç›®æè¿°ï¼šNone  \\nurl: https://github.com/Thinklab-SJTU/DriveTransformer  \\n\\n**LinSATNet**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š73  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-03T01:46:56Z  \\né¡¹ç›®æè¿°ï¼šOfficial implementation of our ICML 2023 paper \"LinSATNet: The Positive Linear Satisfiability Neural Networks\".  \\nurl: https://github.com/Thinklab-SJTU/LinSATNet  \\n\\n**T2TCO**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š65  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T08:55:28Z  \\né¡¹ç›®æè¿°ï¼š[NeurIPS 2023] T2T: From Distribution Learning in Training to Gradient Search in Testing for Combinatorial Optimization  \\nurl: https://github.com/Thinklab-SJTU/T2TCO  \\n\\n**S2TLD**  \\nåˆ†ç±»ï¼šæ•°æ®é›†  \\nstaræ•°å­—ï¼š59  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-05-13T02:06:16Z  \\né¡¹ç›®æè¿°ï¼šNewly released traffic light dataset for small object detection.  \\nurl: https://github.com/Thinklab-SJTU/S2TLD  \\n\\n**ML4CO-Kit**  \\nåˆ†ç±»ï¼šå·¥å…·  \\nstaræ•°å­—ï¼š58  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-02T22:18:52Z  \\né¡¹ç›®æè¿°ï¼šA Python toolkit for Machine Learning (ML) practices for Combinatorial Optimization (CO).  \\nurl: https://github.com/Thinklab-SJTU/ML4CO-Kit  \\n\\n**DriveMoE**  \\nåˆ†ç±»ï¼šæ¨¡å‹  \\nstaræ•°å­—ï¼š89  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-04T02:44:18Z  \\né¡¹ç›®æè¿°ï¼šDrive-Pi0 and DriveMoE on End-to-end Autonomous Driving  \\nurl: https://github.com/Thinklab-SJTU/DriveMoE'),\n",
       " Document(metadata={'source': 'Github\\\\TigerResearch_results.txt'}, page_content='**TigerBot**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹  \\nstaræ•°å­—ï¼š2258  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-18  \\né¡¹ç›®æè¿°ï¼šTigerBot: A multi-language multi-task LLM  \\nurl: https://github.com/TigerResearch/TigerBot  \\n\\n'),\n",
       " Document(metadata={'source': 'Github\\\\USTCAGI_results.txt'}, page_content=''),\n",
       " Document(metadata={'source': 'Github\\\\VISION-SJTU_results.txt'}, page_content=\"**PillarNet-LTS**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆ3Dç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š223  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\né¡¹ç›®æè¿°ï¼š[ECCV 2022] PillarNet: Real-Time and High-Performance Pillar-based 3D Object Detection  \\nurl: https://github.com/VISION-SJTU/PillarNet-LTS  \\n\\n**RECCE**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆäººè„¸ä¼ªé€ æ£€æµ‹æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š147  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-09-06  \\né¡¹ç›®æè¿°ï¼š[CVPR2022] End-to-End Reconstruction-Classification Learning for Face Forgery Detection  \\nurl: https://github.com/VISION-SJTU/RECCE  \\n\\n**Lightning-NeRF**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆç¥ç»è¾å°„åœºæ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š117  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-06  \\né¡¹ç›®æè¿°ï¼š[ICRA 2024] Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous Driving  \\nurl: https://github.com/VISION-SJTU/Lightning-NeRF  \\n\\n**PointAugmenting**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆ3Dç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š114  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-27  \\né¡¹ç›®æè¿°ï¼š[CVPR2021] PointAugmenting: Cross-Modal Augmentation for 3D Object Detection  \\nurl: https://github.com/VISION-SJTU/PointAugmenting  \\n\\n**3dSwap**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆ3Däººè„¸äº¤æ¢æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š86  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-11  \\né¡¹ç›®æè¿°ï¼š[CVPR 2023] 3D-Aware Face Swapping  \\nurl: https://github.com/VISION-SJTU/3dSwap  \\n\\n**USOT**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆè§†è§‰ç›®æ ‡è·Ÿè¸ªæ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š64  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-20  \\né¡¹ç›®æè¿°ï¼š[ICCV2021] Learning to Track Objects from Unlabeled Videos  \\nurl: https://github.com/VISION-SJTU/USOT  \\n\\n**SparseOcc**  \\nåˆ†ç±»ï¼š1. æ¨¡å‹ï¼ˆè¯­ä¹‰å æ®é¢„æµ‹æ¨¡å‹ï¼‰  \\nstaræ•°å­—ï¼š64  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-08-30  \\né¡¹ç›®æè¿°ï¼šOfficial implementation for 'SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction' (CVPR 2024)  \\nurl: https://github.com/VISION-SJTU/SparseOcc  \\n\\n**IoUattack**  \\nåˆ†ç±»ï¼š3. å·¥å…·ï¼ˆå¯¹æŠ—æ”»å‡»å·¥å…·ï¼‰  \\nstaræ•°å­—ï¼š53  \\næœ€åæ›´æ–°æ—¶é—´ï¼š2025-07-08  \\né¡¹ç›®æè¿°ï¼š[CVPR2021] IoU Attack: Towards Temporally Coherent Black-Box Adversarial Attack for Visual Object Tracking  \\nurl: https://github.com/VISION-SJTU/IoUattack  \\n\\næ³¨ï¼šæ‰€æœ‰é¡¹ç›®çš„updated_atéƒ½åœ¨2023å¹´ä¹‹åï¼Œå› æ­¤æ²¡æœ‰é¡¹ç›®è¢«çœç•¥ã€‚\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T05:02:02.330884Z",
     "start_time": "2025-09-08T05:02:02.285513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "csvparser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instruction = \"æ‚¨çš„å“åº”åº”è¯¥æ˜¯csvæ ¼å¼çš„é€—å·åˆ†éš”å€¼çš„åˆ—è¡¨ï¼Œå¿…é¡»æ˜¯ï¼š`åˆ†ç±»ï¼Œurl, é¡¹ç›®åç§°, staræ•°é‡`ã€‚ä½ çš„å›ç­”å¿…é¡»åªåŒ…å«æ¯åˆ—çš„æ ‡é¢˜ä»¥åŠcsvæ–‡ä»¶ä¸­çš„å†…å®¹ï¼Œæ²¡æœ‰ä»»ä½•å…¶ä»–ä¿¡æ¯ï¼Œä¾‹å¦‚\\'\\'\\' å’Œ csvã€‚\""
   ],
   "id": "e5ccd7ead8bc5045",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T05:02:04.901863Z",
     "start_time": "2025-09-08T05:02:04.885840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "è¯·æŸ¥çœ‹ä»¥ä¸‹å†…å®¹å¹¶å›ç­”é—®é¢˜ï¼š\n",
    "\n",
    "{context}\n",
    "\n",
    "è¯·åˆ†ç±»æ¯ä¸€ä¸ªé¡¹ç›®ï¼Œç›¸åŒç±»åˆ«çš„å†™åœ¨ä¸€èµ·ï¼Œä¸åŒç±»åˆ«ç›´æ¥ç©ºå¼€ä¸€è¡Œã€‚\n",
    "\n",
    "{format_instruction}\n",
    "\"\"\")"
   ],
   "id": "b84435dc0303bb88",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T05:02:12.809253Z",
     "start_time": "2025-09-08T05:02:06.739049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = prompt | model | csvparser\n",
    "result = chain.invoke({\"context\":docs[0],\"format_instruction\":format_instruction})\n",
    "print(result)"
   ],
   "id": "2a8ab4456d391170",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', '2. æ•°æ®é›†', 'https://github.com/4paradigm/OpenMLDB', 'OpenMLDB', '1662', '3. å·¥å…·', 'https://github.com/4paradigm/k8s-vgpu-scheduler', 'k8s-vgpu-scheduler', '572', '3. å·¥å…·', 'https://github.com/4paradigm/AutoX', 'AutoX', '539', '4. å…¶ä»–', 'https://github.com/4paradigm/openaios-platform', 'openaios-platform', '98', '4. å…¶ä»–', 'https://github.com/4paradigm/pafka', 'pafka', '67']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T05:05:15.758589Z",
     "start_time": "2025-09-08T05:02:57.984371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "for doc in docs:\n",
    "    chain = prompt | model | csvparser\n",
    "    result = chain.invoke({\"context\":doc,\"format_instruction\":format_instruction})\n",
    "    print(result)\n",
    "\n",
    "\n",
    "    HEADERS = [\"åˆ†ç±»\", \"url\", \"é¡¹ç›®åç§°\", \"staræ•°é‡\"]\n",
    "\n",
    "    def chunk(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i+n]\n",
    "\n",
    "    with open('github_csv.csv', 'a', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # writer.writerow(HEADERS)\n",
    "        for row in chunk(result[len(HEADERS):], len(HEADERS)):\n",
    "            if len(row) < len(HEADERS):  # æœ«å°¾ä¸æ»¡ä¸€è¡Œæ—¶è¡¥ç©º\n",
    "                row += [''] * (len(HEADERS) - len(row))\n",
    "            writer.writerow(row)\n",
    "        writer.writerow(\"\")"
   ],
   "id": "7ac768faa5bc2627",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', '2. æ•°æ®é›†', 'https://github.com/4paradigm/OpenMLDB', 'OpenMLDB', '1662', '3. å·¥å…·', 'https://github.com/4paradigm/k8s-vgpu-scheduler', 'k8s-vgpu-scheduler', '572', '3. å·¥å…·', 'https://github.com/4paradigm/AutoX', 'AutoX', '539', '4. å…¶ä»–', 'https://github.com/4paradigm/openaios-platform', 'openaios-platform', '98', '4. å…¶ä»–', 'https://github.com/4paradigm/pafka', 'pafka', '67']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', '1. æ¨¡å‹', 'https://github.com/AgibotTech/agibot_x1_infer', 'agibot_x1_infer', '1717', '1. æ¨¡å‹', 'https://github.com/AgibotTech/agibot_x1_train', 'agibot_x1_train', '1563', '1. æ¨¡å‹', 'https://github.com/AgibotTech/EnerVerse-AC', 'EnerVerse-AC', '111', '4. å…¶ä»–', 'https://github.com/AgibotTech/agibot_x1_hardware', 'agibot_x1_hardware', '978', '4. å…¶ä»–', 'https://github.com/AgibotTech/Genie-Envisioner', 'Genie-Envisioner', '240', '3. å·¥å…·', 'https://github.com/AgibotTech/genie_sim', 'genie_sim', '273', '2. æ•°æ®é›†', 'https://github.com/AgibotTech/EWMBench', 'EWMBench', '82']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'å·¥å…·', 'https://github.com/FFTAI/teleoperation', 'teleoperation', '122', 'å·¥å…·', 'https://github.com/FFTAI/Wiki-GRx-Deploy', 'Wiki-GRx-Deploy', '112', 'å·¥å…·', 'https://github.com/FFTAI/Wiki-GRx-MJCF', 'Wiki-GRx-MJCF', '53', 'å…¶ä»–', 'https://github.com/FFTAI/fourier-lerobot', 'fourier-lerobot', '67', 'å…¶ä»–', 'https://github.com/FFTAI/Wiki-GRx-Gym', 'Wiki-GRx-Gym', '55', 'å…¶ä»–', 'https://github.com/FFTAI/Wiki-GRx-Pipeline', 'Wiki-GRx-Pipeline', '50']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'æ¨¡å‹', 'https://github.com/fudan-generative-vision/hallo', 'hallo', '8571', 'æ¨¡å‹', 'https://github.com/fudan-generative-vision/champ', 'champ', '4232', 'æ¨¡å‹', 'https://github.com/fudan-generative-vision/hallo2', 'hallo2', '3600', 'æ¨¡å‹', 'https://github.com/fudan-generative-vision/hallo3', 'hallo3', '1302', 'æ¨¡å‹', 'https://github.com/fudan-generative-vision/DicFace', 'DicFace', '435', 'æ•°æ®é›†', 'https://github.com/fudan-generative-vision/dynamicPDB', 'dynamicPDB', '762', 'æ•°æ®é›†', 'https://github.com/fudan-generative-vision/OpenHumanVid', 'OpenHumanVid', '274']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', '1. æ¨¡å‹ï¼ˆä¸­æ–‡æ³•å¾‹å¤§æ¨¡å‹ï¼‰', 'https://github.com/FudanDISC/DISC-LawLLM', 'DISC-LawLLM', '777', '1. æ¨¡å‹ï¼ˆä¸­æ–‡é‡‘èå¤§è¯­è¨€æ¨¡å‹ï¼‰', 'https://github.com/FudanDISC/DISC-FinLLM', 'DISC-FinLLM', '768', '1. æ¨¡å‹ï¼ˆåŒ»ç–—å¤§è¯­è¨€æ¨¡å‹ï¼‰', 'https://github.com/FudanDISC/DISC-MedLLM', 'DISC-MedLLM', '550', '4. å…¶ä»–ï¼ˆç¤¾äº¤æ™ºèƒ½ä½“èµ„æºé›†åˆï¼‰', 'https://github.com/FudanDISC/SocialAgent', 'SocialAgent', '179', '4. å…¶ä»–ï¼ˆæ— æè¿°ï¼Œæš‚å½’ç±»ï¼‰', 'https://github.com/FudanDISC/SocioVerse', 'SocioVerse', '119']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡  ', '4. å…¶ä»–', 'https://github.com/IAAR-Shanghai/SurveyX', 'SurveyX', '898  ', '4. å…¶ä»–', 'https://github.com/IAAR-Shanghai/CTGSurvey', 'CTGSurvey', '185  ', '4. å…¶ä»–', 'https://github.com/IAAR-Shanghai/ICSFSurvey', 'ICSFSurvey', '169  ', '4. å…¶ä»–', 'https://github.com/IAAR-Shanghai/PGRAG', 'PGRAG', '53  ', '3. å·¥å…·', 'https://github.com/IAAR-Shanghai/Awesome-Attention-Heads', 'Awesome-Attention-Heads', '361  ', '3. å·¥å…·', 'https://github.com/IAAR-Shanghai/Meta-Chunking', 'Meta-Chunking', '246  ', '3. å·¥å…·', 'https://github.com/IAAR-Shanghai/xFinder', 'xFinder', '177  ', '3. å·¥å…·', 'https://github.com/IAAR-Shanghai/xVerify', 'xVerify', '128  ', '2. æ•°æ®é›†', 'https://github.com/IAAR-Shanghai/CRUD_RAG', 'CRUD_RAG', '328  ', '2. æ•°æ®é›†', 'https://github.com/IAAR-Shanghai/UHGEval', 'UHGEval', '174  ', '1. æ¨¡å‹', 'https://github.com/IAAR-Shanghai/QAEncoder', 'QAEncoder', '175  ', '1. æ¨¡å‹', 'https://github.com/IAAR-Shanghai/Grimoire', 'Grimoire', '117']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'å…¶ä»–', 'https://github.com/infinigence/Infini-Megrez', 'Infini-Megrez', '326', 'å…¶ä»–', 'https://github.com/infinigence/Infini-Megrez-Omni', 'Infini-Megrez-Omni', '239', 'å·¥å…·', 'https://github.com/infinigence/FlashOverlap', 'FlashOverlap', '161', 'å·¥å…·', 'https://github.com/infinigence/Semi-PD', 'Semi-PD', '107', 'æ•°æ®é›†', 'https://github.com/infinigence/LVEval', 'LVEval', '70']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡  ', '3. å·¥å…·ï¼ˆæœºå™¨äººåŠ¨æ€æ§åˆ¶è½¯ä»¶åŒ…ï¼‰', 'https://github.com/loongOpen/OpenLoong-Dyn-Control', 'OpenLoong-Dyn-Control', '215  ', '3. å·¥å…·ï¼ˆå¼ºåŒ–å­¦ä¹ å¼€å‘å·¥å…·åŒ…ï¼‰', 'https://github.com/loongOpen/Unity-RL-Playground', 'Unity-RL-Playground', '151  ', '3. å·¥å…·ï¼ˆæœºå™¨äººè®­ç»ƒå¹³å°ï¼‰', 'https://github.com/loongOpen/OpenLoong-Gymloong', 'OpenLoong-Gymloong', '70']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡  ', 'å·¥å…·', 'https://github.com/MemTensor/MemOS', 'MemOS', '2413']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'æ¨¡å‹', 'https://github.com/MiniMax-AI/MiniMax-M1', 'MiniMax-M1', '2844', 'æ¨¡å‹', 'https://github.com/MiniMax-AI/MiniMax-01', 'MiniMax-01', '3137', 'å·¥å…·', 'https://github.com/MiniMax-AI/MiniMax-MCP', 'MiniMax-MCP', '932', 'å·¥å…·', 'https://github.com/MiniMax-AI/awesome-minimax-integrations', 'awesome-minimax-integrations', '54', 'å·¥å…·', 'https://github.com/MiniMax-AI/MiniMax-MCP-JS', 'MiniMax-MCP-JS', '81', 'å…¶ä»–', 'https://github.com/MiniMax-AI/One-RL-to-See-Them-All', 'One-RL-to-See-Them-All', '311', 'å…¶ä»–', 'https://github.com/MiniMax-AI/SynLogic', 'SynLogic', '163', 'å…¶ä»–', 'https://github.com/MiniMax-AI/MiniMax-AI.github.io', 'MiniMax-AI.github.io', '52']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'æ¨¡å‹', 'https://github.com/OpenGVLab/InternVL', 'InternVL', '9036', 'æ¨¡å‹', 'https://github.com/OpenGVLab/LLaMA-Adapter', 'LLaMA-Adapter', '5895', 'æ¨¡å‹', 'https://github.com/OpenGVLab/DragGAN', 'DragGAN', '4981', 'æ¨¡å‹', 'https://github.com/OpenGVLab/InternImage', 'InternImage', '2727', 'æ¨¡å‹', 'https://github.com/OpenGVLab/Ask-Anything', 'Ask-Anything', '3297', 'æ¨¡å‹', 'https://github.com/OpenGVLab/InternGPT', 'InternGPT', '3217', 'æ¨¡å‹', 'https://github.com/OpenGVLab/InternVideo', 'InternVideo', '2036', 'æ¨¡å‹', 'https://github.com/OpenGVLab/VisionLLM', 'VisionLLM', '1103', 'æ¨¡å‹', 'https://github.com/OpenGVLab/VideoMamba', 'VideoMamba', '990', 'æ¨¡å‹', 'https://github.com/OpenGVLab/OmniQuant', 'OmniQuant', '845', 'æ¨¡å‹', 'https://github.com/OpenGVLab/SAM-Med2D', 'SAM-Med2D', '1031', 'æ¨¡å‹', 'https://github.com/OpenGVLab/VideoMAEv2', 'VideoMAEv2', '676', 'æ¨¡å‹', 'https://github.com/OpenGVLab/GITM', 'GITM', '633', 'æ¨¡å‹', 'https://github.com/OpenGVLab/UniFormerV2', 'UniFormerV2', '327', 'æ¨¡å‹', 'https://github.com/OpenGVLab/CaFo', 'CaFo', '377', 'æ¨¡å‹', 'https://github.com/OpenGVLab/all-seeing', 'all-seeing', '495', 'æ¨¡å‹', 'https://github.com/OpenGVLab/LAMM', 'LAMM', '317', 'æ¨¡å‹', 'https://github.com/OpenGVLab/unmasked_teacher', 'unmasked_teacher', '337', 'æ¨¡å‹', 'https://github.com/OpenGVLab/Vision-RWKV', 'Vision-RWKV', '493', 'æ¨¡å‹', 'https://github.com/OpenGVLab/M3I-Pretraining', 'M3I-Pretraining', '91', 'æ¨¡å‹', 'https://github.com/OpenGVLab/MM-Interleaved', 'MM-Interleaved', '240', 'æ¨¡å‹', 'https://github.com/OpenGVLab/VideoChat-Flash', 'VideoChat-Flash', '463', 'æ¨¡å‹', 'https://github.com/OpenGVLab/Mono-InternVL', 'Mono-InternVL', '81', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/gv-benchmark', 'gv-benchmark', '189', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/EgoExoLearn', 'EgoExoLearn', '69', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/OmniCorpus', 'OmniCorpus', '391', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/MMT-Bench', 'MMT-Bench', '113', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/MM-NIAH', 'MM-NIAH', '115', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/GUI-Odyssey', 'GUI-Odyssey', '129', 'æ•°æ®é›†', 'https://github.com/OpenGVLab/PhyGenBench', 'PhyGenBench', '117', 'å·¥å…·', 'https://github.com/OpenGVLab/Multi-Modality-Arena', 'Multi-Modality-Arena', '535', 'å·¥å…·', 'https://github.com/OpenGVLab/Instruct2Act', 'Instruct2Act', '369', 'å·¥å…·', 'https://github.com/OpenGVLab/DiffRate', 'DiffRate', '100', 'å·¥å…·', 'https://github.com/OpenGVLab/ControlLLM', 'ControlLLM', '193', 'å·¥å…·', 'https://github.com/OpenGVLab/Awesome-DragGAN', 'Awesome-DragGAN', '85', 'å·¥å…·', 'https://github.com/OpenGVLab/Awesome-LLM4Tool', 'Awesome-LLM4Tool', '68', 'å·¥å…·', 'https://github.com/OpenGVLab/ZeroGUI', 'ZeroGUI', '85', 'å…¶ä»–', 'https://github.com/OpenGVLab/HumanBench', 'HumanBench', '247', 'å…¶ä»–', 'https://github.com/OpenGVLab/UniHCP', 'UniHCP', '156', 'å…¶ä»–', 'https://github.com/OpenGVLab/LORIS', 'LORIS', '61', 'å…¶ä»–', 'https://github.com/OpenGVLab/MUTR', 'MUTR', '82', 'å…¶ä»–', 'https://github.com/OpenGVLab/DDPS', 'DDPS', '72', 'å…¶ä»–', 'https://github.com/OpenGVLab/DCNv4', 'DCNv4', '664', 'å…¶ä»–', 'https://github.com/OpenGVLab/ChartAst', 'ChartAst', '124', 'å…¶ä»–', 'https://github.com/OpenGVLab/Hulk', 'Hulk', '137', 'å…¶ä»–', 'https://github.com/OpenGVLab/PonderV2', 'PonderV2', '359', 'å…¶ä»–', 'https://github.com/OpenGVLab/LCL', 'LCL', '70', 'å…¶ä»–', 'https://github.com/OpenGVLab/EfficientQAT', 'EfficientQAT', '300', 'å…¶ä»–', 'https://github.com/OpenGVLab/Diffree', 'Diffree', '239', 'å…¶ä»–', 'https://github.com/OpenGVLab/MMIU', 'MMIU', '86', 'å…¶ä»–', 'https://github.com/OpenGVLab/vinci', 'vinci', '71', 'å…¶ä»–', 'https://github.com/OpenGVLab/V2PE', 'V2PE', '56', 'å…¶ä»–', 'https://github.com/OpenGVLab/TPO', 'TPO', '58', 'å…¶ä»–', 'https://github.com/OpenGVLab/VideoChat-R1', 'VideoChat-R1', '182', 'å…¶ä»–', 'https://github.com/OpenGVLab/VeBrain', 'VeBrain', '80']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'æ¨¡å‹', 'https://github.com/OpenMOSS/MOSS', 'MOSS', '12060', 'æ¨¡å‹', 'https://github.com/OpenMOSS/MOSS-TTSD', 'MOSS-TTSD', '941', 'æ¨¡å‹', 'https://github.com/OpenMOSS/AnyGPT', 'AnyGPT', '861', 'æ¨¡å‹', 'https://github.com/OpenMOSS/SpeechGPT-2.0-preview', 'SpeechGPT-2.0-preview', '354', 'å·¥å…·', 'https://github.com/OpenMOSS/CoLLiE', 'CoLLiE', '416', 'å·¥å…·', 'https://github.com/OpenMOSS/Language-Model-SAEs', 'Language-Model-SAEs', '146', 'æ•°æ®é›†', 'https://github.com/OpenMOSS/VLABench', 'VLABench', '283', 'æ•°æ®é›†', 'https://github.com/OpenMOSS/HalluQA', 'HalluQA', '132', 'æ•°æ®é›†', 'https://github.com/OpenMOSS/GAOKAO-MM', 'GAOKAO-MM', '65', 'å…¶ä»–', 'https://github.com/OpenMOSS/Say-I-Dont-Know', 'Say-I-Dont-Know', '83', 'å…¶ä»–', 'https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM', 'Thus-Spake-Long-Context-LLM', '56']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡  ', 'æ¨¡å‹ï¼ˆåµŒå…¥æ¨¡å‹ï¼‰', 'https://github.com/OpenSenseNova/piccolo-embedding', 'piccolo-embedding', '138']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'å…¶ä»–', 'https://github.com/stepfun-ai/Step-Audio', 'Step-Audio', '4502', 'å…¶ä»–', 'https://github.com/stepfun-ai/Step-Video-T2V', 'Step-Video-T2V', '3105', 'å…¶ä»–', 'https://github.com/stepfun-ai/NextStep-1', 'NextStep-1', '526', 'å…¶ä»–', 'https://github.com/stepfun-ai/Step3', 'Step3', '418', 'å…¶ä»–', 'https://github.com/stepfun-ai/Step-Video-TI2V', 'Step-Video-TI2V', '352', 'å…¶ä»–', 'https://github.com/stepfun-ai/StepMesh', 'StepMesh', '289', 'æ¨¡å‹', 'https://github.com/stepfun-ai/Step1X-Edit', 'Step1X-Edit', '1615', 'æ¨¡å‹', 'https://github.com/stepfun-ai/Step-Audio2', 'Step-Audio2', '889', 'æ¨¡å‹', 'https://github.com/stepfun-ai/Step1X-3D', 'Step1X-3D', '774']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', 'å…¶ä»–', 'https://github.com/Thinklab-SJTU/Awesome-LLM4AD', 'Awesome-LLM4AD', '1454', 'å…¶ä»–', 'https://github.com/Thinklab-SJTU/awesome-ml4co', 'awesome-ml4co', '1950', 'å…¶ä»–', 'https://github.com/Thinklab-SJTU/Awesome-LLM4EDA', 'Awesome-LLM4EDA', '227', 'å…¶ä»–', 'https://github.com/Thinklab-SJTU/awesome-ai4eda', 'awesome-ai4eda', '177', 'å…¶ä»–', 'https://github.com/Thinklab-SJTU/Bench2Drive-VL', 'Bench2Drive-VL', '150', 'å…¶ä»–', 'https://github.com/Thinklab-SJTU/awesome-molecular-docking', 'awesome-molecular-docking', '102', 'æ•°æ®é›†', 'https://github.com/Thinklab-SJTU/Bench2Drive', 'Bench2Drive', '1613', 'æ•°æ®é›†', 'https://github.com/Thinklab-SJTU/Bench2DriveZoo', 'Bench2DriveZoo', '293', 'æ•°æ®é›†', 'https://github.com/Thinklab-SJTU/S2TLD', 'S2TLD', '59', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/ThinkMatch', 'ThinkMatch', '872', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/R3Det_Tensorflow', 'R3Det_Tensorflow', '546', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/pygmtools', 'pygmtools', '339', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/EDA-AI', 'EDA-AI', '268', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/CSL_RetinaNet_Tensorflow', 'CSL_RetinaNet_Tensorflow', '192', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/PPO-BiHyb', 'PPO-BiHyb', '100', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/T2TCO', 'T2TCO', '65', 'å·¥å…·', 'https://github.com/Thinklab-SJTU/ML4CO-Kit', 'ML4CO-Kit', '58', 'æ¨¡å‹', 'https://github.com/Thinklab-SJTU/Crossformer', 'Crossformer', '601', 'æ¨¡å‹', 'https://github.com/Thinklab-SJTU/DriveTransformer', 'DriveTransformer', '124', 'æ¨¡å‹', 'https://github.com/Thinklab-SJTU/LinSATNet', 'LinSATNet', '73', 'æ¨¡å‹', 'https://github.com/Thinklab-SJTU/DriveMoE', 'DriveMoE', '89']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', '1. æ¨¡å‹', 'https://github.com/TigerResearch/TigerBot', 'TigerBot', '2258']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡']\n",
      "['åˆ†ç±»', 'url', 'é¡¹ç›®åç§°', 'staræ•°é‡', '1. æ¨¡å‹ï¼ˆ3Dç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/PillarNet-LTS', 'PillarNet-LTS', '223', '1. æ¨¡å‹ï¼ˆ3Dç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/PointAugmenting', 'PointAugmenting', '114', '1. æ¨¡å‹ï¼ˆäººè„¸ä¼ªé€ æ£€æµ‹æ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/RECCE', 'RECCE', '147', '1. æ¨¡å‹ï¼ˆç¥ç»è¾å°„åœºæ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/Lightning-NeRF', 'Lightning-NeRF', '117', '1. æ¨¡å‹ï¼ˆ3Däººè„¸äº¤æ¢æ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/3dSwap', '3dSwap', '86', '1. æ¨¡å‹ï¼ˆè§†è§‰ç›®æ ‡è·Ÿè¸ªæ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/USOT', 'USOT', '64', '1. æ¨¡å‹ï¼ˆè¯­ä¹‰å æ®é¢„æµ‹æ¨¡å‹ï¼‰', 'https://github.com/VISION-SJTU/SparseOcc', 'SparseOcc', '64', '3. å·¥å…·ï¼ˆå¯¹æŠ—æ”»å‡»å·¥å…·ï¼‰', 'https://github.com/VISION-SJTU/IoUattack', 'IoUattack', '53']\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
